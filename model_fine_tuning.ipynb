{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_fine_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1anUkYDCUrzy4tPUIjtfi6AxZA-GzgSXK",
      "authorship_tag": "ABX9TyO0m1ejfXP4ahhekkWYHowp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoardoMorucci/Plant-Leaves-Search-Engine---MIRCV/blob/main/model_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook describes the fine-tuning process of Convolutional Neural Network using as Base Network DenseNet"
      ],
      "metadata": {
        "id": "LwmVt8O-Jxhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local download of the dataset"
      ],
      "metadata": {
        "id": "My0VC1BctmJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "from google.colab import files\n",
        "_ = files.upload()\n",
        "\n",
        "! mkdir -p ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "uhlZsAHwts-j",
        "outputId": "e65b6d6a-fc2d-4bb3-e0ac-89716612d5c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f27036fe-2feb-4244-8998-12e8c7a0be70\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f27036fe-2feb-4244-8998-12e8c7a0be70\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d davidedemarco/healthy-unhealthy-plants-dataset-segmented --unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD7UcrOguFHB",
        "outputId": "e0d0f25c-b2e1-4437-8c15-74ef7d6df68e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading healthy-unhealthy-plants-dataset-segmented.zip to /content\n",
            " 98% 632M/642M [00:11<00:00, 63.2MB/s]\n",
            "100% 642M/642M [00:11<00:00, 60.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connection to Google Drive"
      ],
      "metadata": {
        "id": "fGr5XdhuiReN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFiqrSQoiWLs",
        "outputId": "e7b14331-3506-4cd6-d473-230b29c6c023"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "CfFue6RTSsKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Y0z_qR8dSuJ1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n"
      ],
      "metadata": {
        "id": "wAnnPKSJEHSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is on Google Drive and the dataset directory has the structure:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "  class_1/\n",
        "    image_1.jpg\n",
        "    image_2.jpg\n",
        "    ...\n",
        "  class_2/\n",
        "    image_3.jpg\n",
        "    image_4.jpg\n",
        "    ...\n",
        "  ...\n",
        "  ...\n",
        "  class_n/\n",
        "    ...\n",
        "```"
      ],
      "metadata": {
        "id": "w38DOxWrj-Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train and test the model, we need three subsets: train, test and validation. To split the dataset, we use the [split-folder](https://pypi.org/project/split-folders/) package."
      ],
      "metadata": {
        "id": "ej7BE97VFcww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPLHc-B6F20V",
        "outputId": "8d1bc0ed-b181-484c-c18d-bf14107d6f92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.4.3-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to check if the hardware accelaration is enabled, since training a CNN on a CPU could be infeasible."
      ],
      "metadata": {
        "id": "Q6nkuByCSine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check hardware acceleration\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "QAWrdrxOSn6Y",
        "outputId": "5a8d174a-d33b-4a10-e51c-ba1ff441617a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2998ecd6f7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the costants with the directory of the dataset and the directory where the datasplits are created. In addition we define the image size and the batch size."
      ],
      "metadata": {
        "id": "csAKrzcrTymn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"gdrive/Shareddrives/MIRCV-PlantLeavesSearchEngine/\"\n",
        "DATA_DIR = '/content/Healthy-and-Unhealthy-Plants-Dataset-Segmented'\n",
        "SETS_DIR = '/content/healthy-unhealthy-plants-sets'\n",
        "MODEL_DIR = '/content/model'\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 256\n",
        "N_CLASSES = 14"
      ],
      "metadata": {
        "id": "GPllrg4iWfOt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create data splits. The dataset will be divided 80% in training set, 10% in validation set and 10% in test set."
      ],
      "metadata": {
        "id": "Ix4P1D23FfSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "# split data\n",
        "splitfolders.ratio(DATA_DIR, output=SETS_DIR, seed=123, ratio=(0.8, 0.1, 0.1), group_prefix=None)"
      ],
      "metadata": {
        "id": "XKLxtRCsEo-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c09d90-743a-4e93-e30f-3251e72b0ad1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Copying files: 0 files [00:00, ? files/s]\u001b[A\n",
            "Copying files: 98 files [00:00, 978.72 files/s]\u001b[A\n",
            "Copying files: 517 files [00:00, 2866.38 files/s]\u001b[A\n",
            "Copying files: 804 files [00:00, 1572.55 files/s]\u001b[A\n",
            "Copying files: 1007 files [00:00, 1593.30 files/s]\u001b[A\n",
            "Copying files: 1206 files [00:00, 1696.27 files/s]\u001b[A\n",
            "Copying files: 1513 files [00:00, 2068.33 files/s]\u001b[A\n",
            "Copying files: 1744 files [00:00, 1701.63 files/s]\u001b[A\n",
            "Copying files: 2189 files [00:01, 2343.78 files/s]\u001b[A\n",
            "Copying files: 2459 files [00:01, 2117.38 files/s]\u001b[A\n",
            "Copying files: 2698 files [00:01, 2044.88 files/s]\u001b[A\n",
            "Copying files: 2928 files [00:01, 2096.48 files/s]\u001b[A\n",
            "Copying files: 3195 files [00:01, 2243.16 files/s]\u001b[A\n",
            "Copying files: 3432 files [00:01, 2150.76 files/s]\u001b[A\n",
            "Copying files: 3656 files [00:01, 2111.77 files/s]\u001b[A\n",
            "Copying files: 3873 files [00:01, 1865.57 files/s]\u001b[A\n",
            "Copying files: 4068 files [00:02, 1611.71 files/s]\u001b[A\n",
            "Copying files: 4240 files [00:02, 1470.60 files/s]\u001b[A\n",
            "Copying files: 4395 files [00:02, 1108.98 files/s]\u001b[A\n",
            "Copying files: 4558 files [00:02, 1211.45 files/s]\u001b[A\n",
            "Copying files: 4738 files [00:02, 1338.28 files/s]\u001b[A\n",
            "Copying files: 4910 files [00:02, 1429.36 files/s]\u001b[A\n",
            "Copying files: 5067 files [00:02, 1416.11 files/s]\u001b[A\n",
            "Copying files: 5224 files [00:03, 1455.84 files/s]\u001b[A\n",
            "Copying files: 5400 files [00:03, 1531.14 files/s]\u001b[A\n",
            "Copying files: 5582 files [00:03, 1602.18 files/s]\u001b[A\n",
            "Copying files: 5770 files [00:03, 1677.01 files/s]\u001b[A\n",
            "Copying files: 5969 files [00:03, 1763.52 files/s]\u001b[A\n",
            "Copying files: 6168 files [00:03, 1829.19 files/s]\u001b[A\n",
            "Copying files: 6354 files [00:03, 1653.56 files/s]\u001b[A\n",
            "Copying files: 6524 files [00:03, 1582.86 files/s]\u001b[A\n",
            "Copying files: 6686 files [00:03, 1527.60 files/s]\u001b[A\n",
            "Copying files: 6842 files [00:04, 738.05 files/s] \u001b[A\n",
            "Copying files: 7006 files [00:04, 876.36 files/s]\u001b[A\n",
            "Copying files: 7137 files [00:04, 938.11 files/s]\u001b[A\n",
            "Copying files: 7302 files [00:04, 1082.78 files/s]\u001b[A\n",
            "Copying files: 7441 files [00:04, 1141.72 files/s]\u001b[A\n",
            "Copying files: 7591 files [00:04, 1208.92 files/s]\u001b[A\n",
            "Copying files: 7730 files [00:05, 1226.72 files/s]\u001b[A\n",
            "Copying files: 7865 files [00:05, 1234.96 files/s]\u001b[A\n",
            "Copying files: 8124 files [00:05, 1596.94 files/s]\u001b[A\n",
            "Copying files: 8294 files [00:05, 1583.07 files/s]\u001b[A\n",
            "Copying files: 8460 files [00:05, 1572.93 files/s]\u001b[A\n",
            "Copying files: 8623 files [00:05, 1518.65 files/s]\u001b[A\n",
            "Copying files: 8783 files [00:05, 1540.74 files/s]\u001b[A\n",
            "Copying files: 8940 files [00:05, 1522.02 files/s]\u001b[A\n",
            "Copying files: 9095 files [00:05, 1461.66 files/s]\u001b[A\n",
            "Copying files: 9300 files [00:06, 1617.66 files/s]\u001b[A\n",
            "Copying files: 9471 files [00:06, 1640.60 files/s]\u001b[A\n",
            "Copying files: 9637 files [00:06, 1497.84 files/s]\u001b[A\n",
            "Copying files: 9791 files [00:06, 1449.84 files/s]\u001b[A\n",
            "Copying files: 9939 files [00:06, 1291.53 files/s]\u001b[A\n",
            "Copying files: 10073 files [00:06, 1177.46 files/s]\u001b[A\n",
            "Copying files: 10195 files [00:06, 1167.31 files/s]\u001b[A\n",
            "Copying files: 10315 files [00:06, 1131.38 files/s]\u001b[A\n",
            "Copying files: 10447 files [00:06, 1168.79 files/s]\u001b[A\n",
            "Copying files: 10566 files [00:07, 1138.01 files/s]\u001b[A\n",
            "Copying files: 10735 files [00:07, 1277.58 files/s]\u001b[A\n",
            "Copying files: 10896 files [00:07, 1368.78 files/s]\u001b[A\n",
            "Copying files: 11078 files [00:07, 1496.54 files/s]\u001b[A\n",
            "Copying files: 11260 files [00:07, 1569.96 files/s]\u001b[A\n",
            "Copying files: 11419 files [00:07, 1570.85 files/s]\u001b[A\n",
            "Copying files: 11616 files [00:07, 1668.87 files/s]\u001b[A\n",
            "Copying files: 11784 files [00:07, 1528.80 files/s]\u001b[A\n",
            "Copying files: 11940 files [00:08, 1258.65 files/s]\u001b[A\n",
            "Copying files: 12075 files [00:08, 1243.50 files/s]\u001b[A\n",
            "Copying files: 12217 files [00:08, 1287.60 files/s]\u001b[A\n",
            "Copying files: 12351 files [00:08, 702.41 files/s] \u001b[A\n",
            "Copying files: 12751 files [00:08, 1262.40 files/s]\u001b[A\n",
            "Copying files: 12944 files [00:08, 1275.83 files/s]\u001b[A\n",
            "Copying files: 13131 files [00:09, 1396.44 files/s]\u001b[A\n",
            "Copying files: 13378 files [00:09, 1632.50 files/s]\u001b[A\n",
            "Copying files: 13576 files [00:09, 1582.43 files/s]\u001b[A\n",
            "Copying files: 13759 files [00:09, 1620.51 files/s]\u001b[A\n",
            "Copying files: 14084 files [00:09, 2034.24 files/s]\u001b[A\n",
            "Copying files: 14307 files [00:09, 2078.46 files/s]\u001b[A\n",
            "Copying files: 14530 files [00:09, 1916.69 files/s]\u001b[A\n",
            "Copying files: 14734 files [00:09, 1850.48 files/s]\u001b[A\n",
            "Copying files: 14944 files [00:09, 1905.03 files/s]\u001b[A\n",
            "Copying files: 15142 files [00:10, 1886.62 files/s]\u001b[A\n",
            "Copying files: 15336 files [00:10, 1458.74 files/s]\u001b[A\n",
            "Copying files: 15501 files [00:10, 1481.42 files/s]\u001b[A\n",
            "Copying files: 15773 files [00:10, 1753.87 files/s]\u001b[A\n",
            "Copying files: 15962 files [00:10, 1692.86 files/s]\u001b[A\n",
            "Copying files: 16347 files [00:10, 2248.32 files/s]\u001b[A\n",
            "Copying files: 16588 files [00:10, 1970.05 files/s]\u001b[A\n",
            "Copying files: 16802 files [00:10, 1944.55 files/s]\u001b[A\n",
            "Copying files: 17008 files [00:11, 1863.51 files/s]\u001b[A\n",
            "Copying files: 17229 files [00:11, 1951.28 files/s]\u001b[A\n",
            "Copying files: 17431 files [00:11, 1753.63 files/s]\u001b[A\n",
            "Copying files: 17619 files [00:11, 1784.91 files/s]\u001b[A\n",
            "Copying files: 17856 files [00:11, 1940.49 files/s]\u001b[A\n",
            "Copying files: 18057 files [00:11, 1915.62 files/s]\u001b[A\n",
            "Copying files: 18253 files [00:11, 1694.69 files/s]\u001b[A\n",
            "Copying files: 18430 files [00:11, 1688.90 files/s]\u001b[A\n",
            "Copying files: 18604 files [00:12, 1583.93 files/s]\u001b[A\n",
            "Copying files: 18767 files [00:12, 1534.88 files/s]\u001b[A\n",
            "Copying files: 18924 files [00:12, 1504.40 files/s]\u001b[A\n",
            "Copying files: 19077 files [00:12, 934.86 files/s] \u001b[A\n",
            "Copying files: 19518 files [00:12, 1602.27 files/s]\u001b[A\n",
            "Copying files: 19964 files [00:12, 2216.34 files/s]\u001b[A\n",
            "Copying files: 20432 files [00:12, 2795.39 files/s]\u001b[A\n",
            "Copying files: 20920 files [00:12, 3319.52 files/s]\u001b[A\n",
            "Copying files: 21408 files [00:13, 3731.88 files/s]\u001b[A\n",
            "Copying files: 21846 files [00:13, 3908.42 files/s]\u001b[A\n",
            "Copying files: 22269 files [00:13, 3896.26 files/s]\u001b[A\n",
            "Copying files: 22792 files [00:13, 4271.16 files/s]\u001b[A\n",
            "Copying files: 23290 files [00:13, 4472.00 files/s]\u001b[A\n",
            "Copying files: 23795 files [00:13, 4638.98 files/s]\u001b[A\n",
            "Copying files: 24272 files [00:13, 4675.81 files/s]\u001b[A\n",
            "Copying files: 24785 files [00:13, 4809.58 files/s]\u001b[A\n",
            "Copying files: 25272 files [00:13, 4401.17 files/s]\u001b[A\n",
            "Copying files: 25798 files [00:14, 4640.43 files/s]\u001b[A\n",
            "Copying files: 26272 files [00:14, 3981.33 files/s]\u001b[A\n",
            "Copying files: 26762 files [00:14, 4216.50 files/s]\u001b[A\n",
            "Copying files: 27203 files [00:14, 4206.60 files/s]\u001b[A\n",
            "Copying files: 27687 files [00:14, 4378.72 files/s]\u001b[A\n",
            "Copying files: 28136 files [00:14, 3224.58 files/s]\u001b[A\n",
            "Copying files: 28509 files [00:14, 2799.98 files/s]\u001b[A\n",
            "Copying files: 28831 files [00:15, 2670.20 files/s]\u001b[A\n",
            "Copying files: 29126 files [00:15, 2412.95 files/s]\u001b[A\n",
            "Copying files: 29537 files [00:15, 2781.45 files/s]\u001b[A\n",
            "Copying files: 30029 files [00:15, 3287.34 files/s]\u001b[A\n",
            "Copying files: 30564 files [00:15, 3808.12 files/s]\u001b[A\n",
            "Copying files: 31003 files [00:15, 3961.65 files/s]\u001b[A\n",
            "Copying files: 31536 files [00:15, 4337.20 files/s]\u001b[A\n",
            "Copying files: 32114 files [00:15, 4743.75 files/s]\u001b[A\n",
            "Copying files: 32606 files [00:16, 1376.38 files/s]\u001b[A\n",
            "Copying files: 33105 files [00:16, 1756.90 files/s]\u001b[A\n",
            "Copying files: 33684 files [00:16, 2284.37 files/s]\u001b[A\n",
            "Copying files: 34261 files [00:17, 2832.68 files/s]\u001b[A\n",
            "Copying files: 34819 files [00:17, 3336.45 files/s]\u001b[A\n",
            "Copying files: 35331 files [00:17, 3704.01 files/s]\u001b[A\n",
            "Copying files: 35878 files [00:17, 4107.08 files/s]\u001b[A\n",
            "Copying files: 36409 files [00:17, 4402.79 files/s]\u001b[A\n",
            "Copying files: 36955 files [00:17, 4677.82 files/s]\u001b[A\n",
            "Copying files: 37486 files [00:17, 4788.51 files/s]\u001b[A\n",
            "Copying files: 38010 files [00:17, 4421.81 files/s]\u001b[A\n",
            "Copying files: 38540 files [00:17, 4650.53 files/s]\u001b[A\n",
            "Copying files: 39050 files [00:18, 4772.99 files/s]\u001b[A\n",
            "Copying files: 39549 files [00:18, 4691.42 files/s]\u001b[A\n",
            "Copying files: 40034 files [00:18, 4687.25 files/s]\u001b[A\n",
            "Copying files: 40514 files [00:18, 4528.72 files/s]\u001b[A\n",
            "Copying files: 41037 files [00:18, 4723.31 files/s]\u001b[A\n",
            "Copying files: 41589 files [00:18, 4947.72 files/s]\u001b[A\n",
            "Copying files: 42145 files [00:18, 5124.14 files/s]\u001b[A\n",
            "Copying files: 42663 files [00:18, 5109.38 files/s]\u001b[A\n",
            "Copying files: 43178 files [00:18, 5067.04 files/s]\u001b[A\n",
            "Copying files: 43688 files [00:18, 4375.49 files/s]\u001b[A\n",
            "Copying files: 44144 files [00:19, 3677.36 files/s]\u001b[A\n",
            "Copying files: 44675 files [00:19, 4068.38 files/s]\u001b[A\n",
            "Copying files: 45185 files [00:19, 4330.42 files/s]\u001b[A\n",
            "Copying files: 45644 files [00:19, 3536.45 files/s]\u001b[A\n",
            "Copying files: 46037 files [00:19, 3211.71 files/s]\u001b[A\n",
            "Copying files: 46388 files [00:19, 3048.57 files/s]\u001b[A\n",
            "Copying files: 46748 files [00:19, 3176.58 files/s]\u001b[A\n",
            "Copying files: 47084 files [00:20, 2991.85 files/s]\u001b[A\n",
            "Copying files: 47396 files [00:20, 2337.71 files/s]\u001b[A\n",
            "Copying files: 47692 files [00:20, 2471.69 files/s]\u001b[A\n",
            "Copying files: 47964 files [00:20, 2519.47 files/s]\u001b[A\n",
            "Copying files: 48235 files [00:20, 2190.02 files/s]\u001b[A\n",
            "Copying files: 48514 files [00:20, 2319.47 files/s]\u001b[A\n",
            "Copying files: 48763 files [00:20, 2057.70 files/s]\u001b[A\n",
            "Copying files: 48984 files [00:21, 1539.35 files/s]\u001b[A\n",
            "Copying files: 49165 files [00:21, 1560.41 files/s]\u001b[A\n",
            "Copying files: 49341 files [00:21, 1601.30 files/s]\u001b[A\n",
            "Copying files: 49517 files [00:21, 1611.77 files/s]\u001b[A\n",
            "Copying files: 49690 files [00:21, 1526.51 files/s]\u001b[A\n",
            "Copying files: 49902 files [00:21, 1674.15 files/s]\u001b[A\n",
            "Copying files: 50078 files [00:21, 1651.90 files/s]\u001b[A\n",
            "Copying files: 50249 files [00:21, 1526.75 files/s]\u001b[A\n",
            "Copying files: 50452 files [00:22, 1656.70 files/s]\u001b[A\n",
            "Copying files: 50624 files [00:22, 1615.14 files/s]\u001b[A\n",
            "Copying files: 50806 files [00:22, 1669.61 files/s]\u001b[A\n",
            "Copying files: 51125 files [00:22, 2087.29 files/s]\u001b[A\n",
            "Copying files: 51368 files [00:22, 2184.33 files/s]\u001b[A\n",
            "Copying files: 51726 files [00:22, 2587.06 files/s]\u001b[A\n",
            "Copying files: 52157 files [00:22, 3087.11 files/s]\u001b[A\n",
            "Copying files: 52470 files [00:22, 2983.85 files/s]\u001b[A\n",
            "Copying files: 52773 files [00:22, 2786.67 files/s]\u001b[A\n",
            "Copying files: 53359 files [00:23, 3636.08 files/s]\u001b[A\n",
            "Copying files: 53976 files [00:23, 4353.72 files/s]\u001b[A\n",
            "Copying files: 54423 files [00:23, 3980.89 files/s]\u001b[A\n",
            "Copying files: 54835 files [00:23, 3489.20 files/s]\u001b[A\n",
            "Copying files: 55203 files [00:23, 3172.31 files/s]\u001b[A\n",
            "Copying files: 55537 files [00:23, 3026.94 files/s]\u001b[A\n",
            "Copying files: 55851 files [00:23, 2784.48 files/s]\u001b[A\n",
            "Copying files: 56138 files [00:23, 2723.15 files/s]\u001b[A\n",
            "Copying files: 56416 files [00:24, 1859.89 files/s]\u001b[A\n",
            "Copying files: 56672 files [00:24, 1994.81 files/s]\u001b[A\n",
            "Copying files: 56905 files [00:24, 2026.95 files/s]\u001b[A\n",
            "Copying files: 57132 files [00:24, 1934.81 files/s]\u001b[A\n",
            "Copying files: 57342 files [00:24, 1460.55 files/s]\u001b[A\n",
            "Copying files: 57515 files [00:24, 1510.55 files/s]\u001b[A\n",
            "Copying files: 57687 files [00:25, 1498.23 files/s]\u001b[A\n",
            "Copying files: 57870 files [00:25, 1572.39 files/s]\u001b[A\n",
            "Copying files: 58096 files [00:25, 1736.19 files/s]\u001b[A\n",
            "Copying files: 58282 files [00:25, 899.80 files/s] \u001b[A\n",
            "Copying files: 58591 files [00:25, 1250.28 files/s]\u001b[A\n",
            "Copying files: 58917 files [00:25, 1618.62 files/s]\u001b[A\n",
            "Copying files: 59148 files [00:26, 1639.04 files/s]\u001b[A\n",
            "Copying files: 59360 files [00:26, 1720.73 files/s]\u001b[A\n",
            "Copying files: 59762 files [00:26, 2260.40 files/s]\u001b[A\n",
            "Copying files: 60029 files [00:26, 2018.77 files/s]\u001b[A\n",
            "Copying files: 60264 files [00:26, 2063.83 files/s]\u001b[A\n",
            "Copying files: 60494 files [00:26, 1967.35 files/s]\u001b[A\n",
            "Copying files: 60708 files [00:26, 1924.61 files/s]\u001b[A\n",
            "Copying files: 60912 files [00:26, 1738.16 files/s]\u001b[A\n",
            "Copying files: 61096 files [00:27, 1428.31 files/s]\u001b[A\n",
            "Copying files: 61264 files [00:27, 1477.90 files/s]\u001b[A\n",
            "Copying files: 61439 files [00:27, 1532.05 files/s]\u001b[A\n",
            "Copying files: 61618 files [00:27, 1593.12 files/s]\u001b[A\n",
            "Copying files: 61785 files [00:27, 1583.13 files/s]\u001b[A\n",
            "Copying files: 62019 files [00:27, 1787.10 files/s]\u001b[A\n",
            "Copying files: 62225 files [00:27, 1863.04 files/s]\u001b[A\n",
            "Copying files: 62466 files [00:27, 2018.15 files/s]\u001b[A\n",
            "Copying files: 62672 files [00:27, 1816.72 files/s]\u001b[A\n",
            "Copying files: 63029 files [00:28, 2289.39 files/s]\u001b[A\n",
            "Copying files: 63268 files [00:28, 2226.66 files/s]\u001b[A\n",
            "Copying files: 63564 files [00:28, 2428.92 files/s]\u001b[A\n",
            "Copying files: 63915 files [00:28, 2734.11 files/s]\u001b[A\n",
            "Copying files: 64223 files [00:28, 2826.96 files/s]\u001b[A\n",
            "Copying files: 64670 files [00:28, 3304.41 files/s]\u001b[A\n",
            "Copying files: 65188 files [00:28, 3853.53 files/s]\u001b[A\n",
            "Copying files: 65578 files [00:29, 2086.12 files/s]\u001b[A\n",
            "Copying files: 65882 files [00:29, 1977.16 files/s]\u001b[A\n",
            "Copying files: 66146 files [00:29, 1906.67 files/s]\u001b[A\n",
            "Copying files: 66382 files [00:29, 1979.01 files/s]\u001b[A\n",
            "Copying files: 66615 files [00:29, 1869.05 files/s]\u001b[A\n",
            "Copying files: 66913 files [00:29, 2112.75 files/s]\u001b[A\n",
            "Copying files: 67288 files [00:29, 2503.86 files/s]\u001b[A\n",
            "Copying files: 67567 files [00:29, 2479.67 files/s]\u001b[A\n",
            "Copying files: 67835 files [00:30, 2153.21 files/s]\u001b[A\n",
            "Copying files: 68071 files [00:30, 1952.02 files/s]\u001b[A\n",
            "Copying files: 68460 files [00:30, 2388.48 files/s]\u001b[A\n",
            "Copying files: 68743 files [00:30, 2490.56 files/s]\u001b[A\n",
            "Copying files: 69010 files [00:30, 2487.72 files/s]\u001b[A\n",
            "Copying files: 69302 files [00:30, 2601.36 files/s]\u001b[A\n",
            "Copying files: 69573 files [00:30, 2267.47 files/s]\u001b[A\n",
            "Copying files: 69814 files [00:31, 1519.23 files/s]\u001b[A\n",
            "Copying files: 70011 files [00:31, 1602.67 files/s]\u001b[A\n",
            "Copying files: 70234 files [00:31, 1735.86 files/s]\u001b[A\n",
            "Copying files: 70436 files [00:31, 1465.74 files/s]\u001b[A\n",
            "Copying files: 70608 files [00:31, 1422.71 files/s]\u001b[A\n",
            "Copying files: 70850 files [00:31, 1644.65 files/s]\u001b[A\n",
            "Copying files: 71035 files [00:31, 1658.50 files/s]\u001b[A\n",
            "Copying files: 71278 files [00:32, 1853.18 files/s]\u001b[A\n",
            "Copying files: 71477 files [00:32, 1828.26 files/s]\u001b[A\n",
            "Copying files: 71681 files [00:32, 1863.99 files/s]\u001b[A\n",
            "Copying files: 72034 files [00:32, 2226.73 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Tommaso"
      ],
      "metadata": {
        "id": "AxIzzZGUXmbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esegui tutte le celle prima di questa per avere la cartella pronta per essere zippata. Dopo esegui questa per zippare. Ricordati di rinominare la cartella con i trattini bassi come nella variabile DATA_DIR. Buona fortuna."
      ],
      "metadata": {
        "id": "dqjbOjvRXp7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! zip Healthy-and-Unhealthy-Plants-Dataset-Segmented.zip Healthy-and-Unhealthy-Plants-Dataset-Segmented"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPBo-dPFajck",
        "outputId": "e748d4a9-5838-45f9-9e0a-25cce88f3741"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: Healthy-and-Unhealthy-Plants-Dataset-Segmented/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create the Dataset objects from the sets directory. We use the [image_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) function provided by Keras. An example of use of this library can be found on the official documentation provided by Keras ([here](https://keras.io/examples/vision/image_classification_from_scratch/))."
      ],
      "metadata": {
        "id": "ucmQuLtpU7gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/train',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=123,\n",
        "    shuffle=True,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/val',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=123,\n",
        "    shuffle=True,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/test',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=123,\n",
        "    shuffle=True,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# use buffered prefetching so we can yield data \n",
        "# from disk without having I/O becoming blocking\n",
        "train_ds = train_ds.prefetch(buffer_size=BATCH_SIZE)\n",
        "val_ds = val_ds.prefetch(buffer_size=BATCH_SIZE)\n",
        "test_ds = test_ds.prefetch(buffer_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "xExN_Ss3WRLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zip"
      ],
      "metadata": {
        "id": "WMAeUURrX4tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The images needs to be preprocessed before going in input to the CNN DenseNet. We use the function [tf.keras.applications.densenet.preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/densenet/preprocess_input) to preprocess the image. In addition we add the batch dimension."
      ],
      "metadata": {
        "id": "lRYyCnb7iJ4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(images, labels):\n",
        "  images = tf.keras.applications.densenet.preprocess_input(images)\n",
        "  return images, labels\n",
        "  \n",
        "#preprocessing of the images in all the set\n",
        "train_ds = train_ds.map(preprocess)\n",
        "val_ds = val_ds.map(preprocess)\n",
        "test_ds = test_ds.map(preprocess)"
      ],
      "metadata": {
        "id": "dVGLR6_FiGRJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "juFJRQj9cFc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN used has base network is DenseNet. Since we want to fine-tune the network. We remove the fully-connected layer on top and later we will add an output layer with 14 neurons (1 for each class we want to predict)."
      ],
      "metadata": {
        "id": "YEgHoH2ycH1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = tf.keras.applications.DenseNet121(\n",
        "    input_shape = (224, 224, 3),\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,  # do not include the pretrained layers implementing the imagenet classifier\n",
        ")\n",
        "\n",
        "# freezes weights of all levels of the pre-trained network\n",
        "pretrained_model.trainable = False \n",
        "\n",
        "#pretrained_model.summary()"
      ],
      "metadata": {
        "id": "Rl5ZRGzodFTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85623e1a-97df-4fe8-f853-a5ddb8939bf6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On top of the base network we apply global average pooling and we add an hidden classifier with 256 neurons. The last layer of the network is the output classification layer, with 1 neuron for each class and with softmax as activation function."
      ],
      "metadata": {
        "id": "3dijXjtDqO_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers as L\n",
        "\n",
        "x = pretrained_model.output\n",
        "\n",
        "# add a global average pooling\n",
        "x = L.GlobalAveragePooling2D(name='gap')(x)\n",
        "x = L.Flatten(name='flatten')(x)\n",
        "\n",
        "#STRATI COPIATI DA SLIDE, DA CAPIRE LA LORO UTILITA'\n",
        "# add a fully-connected layer (Dense) of 256 neurons with name='classifier_hidden'\n",
        "x = L.Dense(256,activation='relu', name='classifier_hidden')(x)\n",
        "\n",
        "# add output classification layer with n_classes outputs and softmax activation\n",
        "x = L.Dense(N_CLASSES, activation='softmax')(x)\n",
        "new_output = x\n",
        "\n",
        "model = tf.keras.models.Model(inputs=pretrained_model.input, outputs=new_output, name='healthy_and_unhealty_plants_classifier')\n",
        "\n",
        "#model.summary()"
      ],
      "metadata": {
        "id": "hU1Anl6qqQSl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prevent huge gradients coming from the newly initialized layers from destroying the weights in the pretrained layers  we will initially freeze the layers of the base network and train only new layers. As optimizers we use Adam."
      ],
      "metadata": {
        "id": "jzPtHCanx2py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.01\n",
        "epochs=6\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(optimizer,\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=[\"accuracy\"])\n",
        "callbacks = [\n",
        "  # early stopping\n",
        "      tf.keras.callbacks.EarlyStopping(\n",
        "          monitor='val_loss', \n",
        "          patience=2,\n",
        "          restore_best_weights=True),\n",
        "\n",
        "  # checkpoint best model \n",
        "  tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=MODEL_DIR + \"healthy_and_unhealty_plants_classifier\",\n",
        "    save_weights_only=True,\n",
        "    monitor='accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        "  ),\n",
        "]\n",
        "\n",
        "train_ds_shuffle = train_ds.shuffle(123)  # shuffles data each epoch\n",
        "\n",
        "# train the model\n",
        "history = model.fit(\n",
        "  train_ds_shuffle,\n",
        "  validation_data=val_ds,\n",
        "  epochs = epochs,  \n",
        "  callbacks=callbacks,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B63WEshrz6Av",
        "outputId": "1b060743-409a-49b2-f6a1-7c297cb397e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n"
          ]
        }
      ]
    }
  ]
}