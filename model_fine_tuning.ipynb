{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_fine_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1anUkYDCUrzy4tPUIjtfi6AxZA-GzgSXK",
      "authorship_tag": "ABX9TyPT5Xl6fE7cEr+Hi6YnddrR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoardoMorucci/Plant-Leaves-Search-Engine---MIRCV/blob/main/model_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook describes the fine-tuning process of Convolutional Neural Network using as Base Network DenseNet"
      ],
      "metadata": {
        "id": "LwmVt8O-Jxhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connection to Google Drive"
      ],
      "metadata": {
        "id": "fGr5XdhuiReN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFiqrSQoiWLs",
        "outputId": "8fa16ee4-e9ee-468c-fd12-3c2b8938997c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "CfFue6RTSsKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Y0z_qR8dSuJ1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n"
      ],
      "metadata": {
        "id": "wAnnPKSJEHSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is on Google Drive and the dataset directory has the structure:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "  class_1/\n",
        "    image_1.jpg\n",
        "    image_2.jpg\n",
        "    ...\n",
        "  class_2/\n",
        "    image_3.jpg\n",
        "    image_4.jpg\n",
        "    ...\n",
        "  ...\n",
        "  ...\n",
        "  class_n/\n",
        "    ...\n",
        "```"
      ],
      "metadata": {
        "id": "w38DOxWrj-Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train and test the model, we need three subsets: train, test and validation. To split the dataset, we use the [split-folder](https://pypi.org/project/split-folders/) package."
      ],
      "metadata": {
        "id": "ej7BE97VFcww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPLHc-B6F20V",
        "outputId": "6022c24a-09c1-4e47-b451-77cc7032168b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.4.3-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to check if the hardware accelaration is enabled, since training a CNN on a CPU could be infeasible."
      ],
      "metadata": {
        "id": "Q6nkuByCSine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check hardware acceleration\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAWrdrxOSn6Y",
        "outputId": "1a31b6c0-eb72-44d4-e45c-87e41847fd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the costants with the directory of the dataset and the directory where the datasplits are created. In addition we define the image size and the batch size."
      ],
      "metadata": {
        "id": "csAKrzcrTymn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"gdrive/Shareddrives/MIRCV-PlantLeavesSearchEngine/\"\n",
        "DATA_DIR = '/content/Healthy-and-Unhealthy-Plants-Dataset-Segmented'\n",
        "SETS_DIR = '/content/healthy-unhealthy-plants-sets'\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 256\n"
      ],
      "metadata": {
        "id": "GPllrg4iWfOt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create data splits. The dataset will be divided 80% in training set, 10% in validation set and 10% in test set."
      ],
      "metadata": {
        "id": "Ix4P1D23FfSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders\n",
        "# split data\n",
        "splitfolders.ratio(DATA_DIR, output=SETS_DIR, seed=123, ratio=(0.8, 0.1, 0.1), group_prefix=None)"
      ],
      "metadata": {
        "id": "XKLxtRCsEo-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9904d71b-fc2e-45b6-ebc4-4da2298539d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 72034 files [00:18, 3871.66 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create the Dataset objects from the sets directory. We use the [image_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) function provided by Keras. An example of use of this library can be found on the official documentation provided by Keras ([here](https://keras.io/examples/vision/image_classification_from_scratch/))."
      ],
      "metadata": {
        "id": "ucmQuLtpU7gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/train',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=123,\n",
        "    shuffle=True,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/val',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=123,\n",
        "    shuffle=True,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    SETS_DIR + '/test',\n",
        "    labels='inferred', #the label of the dataset is obtained by the name of the directory\n",
        "    seed=123,\n",
        "    shuffle=True,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xExN_Ss3WRLo",
        "outputId": "793642c7-f345-41bb-e6a5-3136be4308be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 57623 files belonging to 14 classes.\n",
            "Found 7195 files belonging to 14 classes.\n",
            "Found 7216 files belonging to 14 classes.\n"
          ]
        }
      ]
    }
  ]
}